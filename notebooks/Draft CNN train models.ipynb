{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from classification_models.tfkeras import Classifiers\n",
    "import efficientnet.tfkeras as efn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import time\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras.metrics import AUC\n",
    "\n",
    "from efficientnet.tfkeras import EfficientNetB3\n",
    "from efficientnet.tfkeras import EfficientNetB2\n",
    "#from tensorflow.python.keras.applications import InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "config = tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avoid tensorflow to get all gpu memory\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "    for gpu in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "except:\n",
    "    logger.info('error accessing gpu devices...')\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collecting information to use in models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I create a dataframe with class and file paths for all images on train folder\n",
    "root_train = '../data/raw/vehicle/train/train/'\n",
    "data_train = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in sorted(os.listdir(root_train)):\n",
    "    for file in sorted(os.listdir(os.path.join(root_train, category))):\n",
    "        data_train.append((category, os.path.join(root_train, category, file)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(data_train, columns=['class','file_path']).sample(frac=1.0)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(train_df)} images on train folder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I create a dataframe with file paths for all images on test folder\n",
    "root_test = '../data/raw/vehicle/test/testset/'\n",
    "data_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in sorted(os.listdir(root_test)):\n",
    "    data_test.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(data_test, columns=['file_path'])\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'There are {len(test_df)} images on test folder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc = AUC(name='auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df['file_path']\n",
    "y = train_df['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "networks=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalTime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = []\n",
    "best_auc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% K-folds to files so that they can be loaded for different networks\n",
    "i = 0\n",
    "for train_index, test_index in skf.split(X,y):\n",
    "    \n",
    "    trainName= f\"../data/processed/data_train_fold{i}\"\n",
    "    valName = f\"../data/processed/data_val_fold{i}\"\n",
    "        \n",
    "    data_train = train_df.iloc[train_index]\n",
    "    data_test = train_df.iloc[test_index]\n",
    "    \n",
    "    data_train.to_pickle(trainName)\n",
    "    data_test.to_pickle(valName)\n",
    "        \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,2):\n",
    "    \n",
    "    print(\"##################################\")\n",
    "    print(f\"FOLD {i}\")\n",
    "    print(\"##################################\")\n",
    "          \n",
    "    trainName= f\"../data/processed/data_train_fold{i}\"\n",
    "    valName = f\"../data/processed/data_val_fold{i}\"\n",
    "    \n",
    "    print(f\"Loading: {trainName}\")    \n",
    "    data_train = pd.read_pickle(trainName)\n",
    "    \n",
    "    print(f\"Loading: {valName}\")    \n",
    "    data_test = pd.read_pickle(valName)\n",
    "             \n",
    "    Teaching_time = time.time()\n",
    "           \n",
    "    valdatagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    traindatagen = ImageDataGenerator(\n",
    "            rescale = 1./255,\n",
    "            horizontal_flip = True,\n",
    "            rotation_range = 25,\n",
    "            height_shift_range = 0.2,\n",
    "            width_shift_range = 0.2,\n",
    "            zoom_range = 0.2,\n",
    "            shear_range = 0.2,\n",
    "            brightness_range = (0.9, 1.1)\n",
    "            )\n",
    "            \n",
    "    train_generator = traindatagen.flow_from_dataframe(\n",
    "            dataframe = data_train,\n",
    "            x_col = 'file_path',\n",
    "            y_col = 'class',\n",
    "            target_size = (224,224),\n",
    "            batch_size = 50            \n",
    "            )\n",
    "    \n",
    "    val_generator = valdatagen.flow_from_dataframe(\n",
    "            dataframe = data_test,\n",
    "            x_col = 'file_path',\n",
    "            y_col = 'class',\n",
    "            target_size = (224,224),\n",
    "            batch_size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Change parameters as needed, trying to keep things simple\n",
    "\n",
    "for i in range(0,2):\n",
    "\n",
    "    K.clear_session()    \n",
    "    \n",
    "    print(\"##################################\")\n",
    "    print(f\"FOLD {i}\")\n",
    "    print(\"##################################\")\n",
    "          \n",
    "    trainName= f\"../data/processed/data_train_fold{i}\"\n",
    "    valName = f\"../data/processed/data_val_fold{i}\"\n",
    "    \n",
    "    print(f\"Loading: {trainName}\")    \n",
    "    data_train = pd.read_pickle(trainName)\n",
    "    \n",
    "    print(f\"Loading: {valName}\")    \n",
    "    data_test = pd.read_pickle(valName)\n",
    "             \n",
    "    Teaching_time = time.time()\n",
    "           \n",
    "    valdatagen = ImageDataGenerator(rescale=1./255)\n",
    "    \n",
    "    traindatagen = ImageDataGenerator(\n",
    "            rescale = 1./255,\n",
    "            horizontal_flip = True,\n",
    "            rotation_range = 25,\n",
    "            height_shift_range = 0.2,\n",
    "            width_shift_range = 0.2,\n",
    "            zoom_range = 0.2,\n",
    "            shear_range = 0.2,\n",
    "            brightness_range = (0.9, 1.1)\n",
    "            )\n",
    "            \n",
    "    train_generator = traindatagen.flow_from_dataframe(\n",
    "            dataframe = data_train,\n",
    "            x_col = 'file_path',\n",
    "            y_col = 'class',\n",
    "            target_size = (224,224),\n",
    "            batch_size = 50            \n",
    "            )\n",
    "    \n",
    "    val_generator = valdatagen.flow_from_dataframe(\n",
    "            dataframe = data_test,\n",
    "            x_col = 'file_path',\n",
    "            y_col = 'class',\n",
    "            target_size = (224,224),\n",
    "            batch_size = 50            \n",
    "            )\n",
    "    \n",
    "    pre_network = EfficientNetB3(include_top = False, \n",
    "                             weights = 'imagenet', \n",
    "                             input_shape = (224,224,3)) \n",
    "    network = models.Sequential()\n",
    "    network.add(pre_network)\n",
    "    network.add(layers.GlobalMaxPooling2D())\n",
    "    network.add(layers.Dropout(rate=0.2))\n",
    "    network.add(layers.Dense(17, activation = 'softmax',kernel_initializer = 'he_uniform'))\n",
    "    \n",
    "    network.compile(loss = 'categorical_crossentropy',\n",
    "                optimizer = optimizers.RMSprop(lr=1e-5),\n",
    "                metrics = ['acc', auc])\n",
    "    \n",
    "    network.summary()\n",
    "    \n",
    "    train_steps = int(np.ceil( len(data_train) / 50))\n",
    "    \n",
    "    val_steps = int(np.ceil(len(data_test) / 50))\n",
    "    \n",
    "    storage_location = \"best_acc.hdf5\"\n",
    "    \n",
    "    auc_recording = \"best_auc.hdf5\"\n",
    "    \n",
    "    recording = ModelCheckpoint(monitor = 'val_acc',  \n",
    "                            mode = 'max', \n",
    "                            filepath = storage_location, \n",
    "                            verbose = 1, \n",
    "                            save_weights_only = True, \n",
    "                            save_best_only=True)\n",
    "    \n",
    "    auc_recording = ModelCheckpoint(monitor = 'val_auc',  \n",
    "                                mode = 'max', \n",
    "                                filepath = auc_recording, \n",
    "                                verbose = 1, \n",
    "                                save_weights_only = True, \n",
    "                                save_best_only=True)\n",
    "    \n",
    "    intermediary =  ReduceLROnPlateau(monitor='val_acc', \n",
    "                               factor=0.5, \n",
    "                               patience=3, \n",
    "                               verbose=1)\n",
    "    \n",
    "    stop = EarlyStopping(monitor = 'val_acc', \n",
    "                     patience = 10, \n",
    "                     verbose = 1)\n",
    "    \n",
    "    # Easy way to fight the class imbalance using sklearn \n",
    "    class_weights_lst = class_weight.compute_class_weight('balanced', \n",
    "                                                      np.unique(train_generator.classes), \n",
    "                                                      train_generator.classes)\n",
    "    class_weights = dict(zip(np.unique(train_generator.classes), class_weights_lst))\n",
    "    \n",
    "    history = network.fit(train_generator, \n",
    "                      steps_per_epoch = train_steps, \n",
    "                      epochs = 100, \n",
    "                      validation_data = val_generator, \n",
    "                      validation_steps = val_steps, \n",
    "                      verbose = 1, \n",
    "                      callbacks = [recording, intermediary, auc_recording, stop], \n",
    "                      workers = 6, \n",
    "                      max_queue_size = 64, \n",
    "                      class_weight = class_weights\n",
    "                      )\n",
    "    \n",
    "    bestauc = max(history.history['val_auc'])\n",
    "    \n",
    "    best_auc.append(bestauc)\n",
    "    \n",
    "    auc_name = f\"01_EffiNetB3_auc_fold_{i}\"\n",
    "    print(f\"saving: {aucname}\")\n",
    "    \n",
    "    network.load_weights(auc_recording)\n",
    "    network.save(aucname)\n",
    "    \n",
    "    network.load_weights(storage_location)\n",
    "    \n",
    "    bestacc = max(history.history['val_acc'])\n",
    "    best_acc.append(bestacc)\n",
    "    \n",
    "    name = f\"01_EffiNetB3_acc_foldi_{i}.hdf5\"\n",
    "    print(f\"saving: {name}\")\n",
    "    network.save(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing some models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple model: Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple model\n",
    "model = Sequential([Conv2D(16, 3, padding='same', activation='relu', input_shape=(224, 224, 3)),\n",
    "                    MaxPooling2D(),\n",
    "                    Dropout(0.2),\n",
    "                    Conv2D(32, 3, padding='same', activation='relu'),\n",
    "                    MaxPooling2D(),\n",
    "                    Conv2D(64, 3, padding='same', activation='relu'),\n",
    "                    MaxPooling2D(),\n",
    "                    Dropout(0.2),\n",
    "                    Flatten(),\n",
    "                    Dense(512, activation='relu'),\n",
    "                    Dense(17, activation='softmax')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "optimizer = Adam(lr=0.0001)\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class weights\n",
    "class_weights_lst = class_weight.compute_class_weight('balanced', \n",
    "                                                  np.unique(train_generator.classes), \n",
    "                                                  train_generator.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights = dict(zip(np.unique(train_generator.classes), class_weights_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop training when a monitored quantity has stopped improving\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=5, verbose=1, min_delta=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if training does not improve after specific epochs, reduce the learning rate value by improving training\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.1, patience=4, verbose=1, min_delta=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "file_path = f'../data/results/{model.name}.h5'\n",
    "best_model = ModelCheckpoint(file_path, \n",
    "                             save_best_only=True, \n",
    "                             monitor='val_accuracy',\n",
    "                             verbose=1,\n",
    "                             save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "history = model.fit(train_generator,\n",
    "                    steps_per_epoch=22436 // 32,\n",
    "                    epochs=100,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=5609 // 32,\n",
    "                    class_weight=class_weights,\n",
    "                    callbacks=[earlyStopping, reduce_lr, best_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy & validation accuracy\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lossvalues & validation lossvalues\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training\n",
    "epochs_range = range(100)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.title('Training and Validation Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet50 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.resnet50.ResNet50(include_top=False, \n",
    "                                                     weights='imagenet', \n",
    "                                                     pooling='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(base_model)\n",
    "model2.add(Dense(1024, activation='relu'))\n",
    "model2.add(Dense(17, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model2.compile(optimizer='adam', \n",
    "               loss='sparse_categorical_crossentropy', \n",
    "               metrics=['accuracy'])\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "file_path = model2.name + '.{epoch:02d}-{loss:.2f}-{accuracy:.2f}.hdf5'\n",
    "best_model2 = tf.keras.callbacks.ModelCheckpoint(file_path, save_best_only=False, monitor='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "history2 = model2.fit(train_generator,\n",
    "                    steps_per_epoch=22443 // 50,\n",
    "                    epochs=100,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=5602 // 50,\n",
    "                    class_weight=class_weights,\n",
    "                    callbacks=[earlyStopping, best_model2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy & validation accuracy\n",
    "acc2 = history2.history2['accuracy']\n",
    "val_acc2 = history2.history2['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lossvalues & validation lossvalues\n",
    "loss2 = history2.history2['loss']\n",
    "val_loss2 = history2.history2['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training\n",
    "epochs_range = range(100)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc2, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc2, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss2, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss2, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.title('Training and Validation Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MobileNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basemodel, _ = Classifiers.get('mobilenetv2')\n",
    "\n",
    "base_model3 = basemodel(input_shape=(224, 224, 3),\n",
    "                       weights='imagenet',\n",
    "                       include_top=False)\n",
    "x = GlobalAveragePooling2D()(base_model3.output)\n",
    "output = Dense(17, activation='softmax')(x)\n",
    "model3 = Model(inputs=[base_model3.input], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model3.compile(optimizer='adam', \n",
    "               loss='sparse_categorical_crossentropy', \n",
    "               metrics=['accuracy'])\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "file_path = model3.name + '.{epoch:02d}-{loss:.2f}-{accuracy:.2f}.hdf5'\n",
    "best_model3 = tf.keras.callbacks.ModelCheckpoint(file_path, save_best_only=False, monitor='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "history3 = model3.fit(train_generator,\n",
    "                    steps_per_epoch=22443 // 50,\n",
    "                    epochs=100,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=5602 // 50,\n",
    "                    class_weight=class_weights,\n",
    "                    callbacks=[earlyStopping, best_model3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy & validation accuracy\n",
    "acc3 = history3.history3['accuracy']\n",
    "val_acc3 = history3.history3['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lossvalues & validation lossvalues\n",
    "loss3 = history3.history3['loss']\n",
    "val_loss3 = history3.history3['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training\n",
    "epochs_range = range(100)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc3, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc3, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss3, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss3, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.title('Training and Validation Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NasNet_keras model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model4 = tf.keras.applications.nasnet.NASNetLarge(include_top =False,\n",
    "                                                       input_shape=(331, 331, 3),\n",
    "                                                       weights='imagenet', \n",
    "                                                       pooling='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(base_model4)\n",
    "model4.add(Dense(127, activation='relu'))\n",
    "model4.add(Dense(17, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model4.compile(optimizer='adam', \n",
    "               loss='sparse_categorical_crossentropy', \n",
    "               metrics=['accuracy'])\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "file_path = model4.name + '.{epoch:02d}-{loss:.2f}-{accuracy:.2f}.hdf5'\n",
    "best_model4 = tf.keras.callbacks.ModelCheckpoint(file_path, save_best_only=False, monitor='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "history4 = model4.fit(train_generator,\n",
    "                    steps_per_epoch=22443 // 50,\n",
    "                    epochs=100,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=5602 // 50,\n",
    "                    class_weight=class_weights,\n",
    "                    callbacks=[earlyStopping, best_model4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy & validation accuracy\n",
    "acc4 = history4.history4['accuracy']\n",
    "val_acc4 = history4.history4['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lossvalues & validation lossvalues\n",
    "loss4 = history4.history4['loss']\n",
    "val_loss4 = history4.history4['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training\n",
    "epochs_range = range(100)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc4, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc4, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss4, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss4, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.title('Training and Validation Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NasNetLarge model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NASNetLarge, _ = Classifiers.get('nasnetlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model5 = NASNetLarge(include_top=False,\n",
    "                          input_shape=(331, 331, 3), \n",
    "                          weights='imagenet')\n",
    "x = GlobalAveragePooling2D()(base_model5.output)\n",
    "output = Dense(17, activation='softmax')(x)\n",
    "model5 = Model(inputs=[base_model5.input], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model5.compile(optimizer='adam', \n",
    "               loss='sparse_categorical_crossentropy', \n",
    "               metrics=['accuracy'])\n",
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "file_path = model5.name + '.{epoch:02d}-{loss:.2f}-{accuracy:.2f}.hdf5'\n",
    "best_model5 = tf.keras.callbacks.ModelCheckpoint(file_path, save_best_only=False, monitor='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "history5 = model5.fit(train_generator,\n",
    "                    steps_per_epoch=22443 // 50,\n",
    "                    epochs=100,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=5602 // 50,\n",
    "                    class_weight=class_weights,\n",
    "                    callbacks=[earlyStopping, best_model5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy & validation accuracy\n",
    "acc5 = history5.history5['accuracy']\n",
    "val_acc5 = history5.history5['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lossvalues & validation lossvalues\n",
    "loss5 = history5.history5['loss']\n",
    "val_loss5 = history5.history5['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training\n",
    "epochs_range = range(100)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc5, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc5, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss5, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss5, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.title('Training and Validation Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EfficientNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model6 = efn.EfficientNetB7(include_top=False, \n",
    "                                input_shape=(224, 224, 3),\n",
    "                                weights='imagenet', \n",
    "                                pooling='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model6 = Sequential()\n",
    "model6.add(base_model6)\n",
    "model6.add(Dense(512, activation='relu'))\n",
    "model6.add(Dense(17, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model6.compile(optimizer='adam', \n",
    "               loss='sparse_categorical_crossentropy', \n",
    "               metrics=['accuracy'])\n",
    "model6.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "file_path = model6.name + '.{epoch:02d}-{loss:.2f}-{accuracy:.2f}.hdf5'\n",
    "best_model6 = tf.keras.callbacks.ModelCheckpoint(file_path, save_best_only=False, monitor='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "history6 = model6.fit(train_generator,\n",
    "                    steps_per_epoch=22443 // 50,\n",
    "                    epochs=100,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=5602 // 50,\n",
    "                    class_weight=class_weights,\n",
    "                    callbacks=[earlyStopping, best_model6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy & validation accuracy\n",
    "acc6 = history6.history6['accuracy']\n",
    "val_acc6 = history6.history6['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lossvalues & validation lossvalues\n",
    "loss6 = history6.history6['loss']\n",
    "val_loss6 = history6.history6['val_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training\n",
    "epochs_range = range(100)\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc5, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc5, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid()\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss5, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss5, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid()\n",
    "plt.title('Training and Validation Loss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ironhack_env]",
   "language": "python",
   "name": "conda-env-ironhack_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
